# -*- coding: utf-8 -*-
"""Machine Learning Water Ph.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19wqj5Xl2tobptBuTVSoD3c1hsV3Uaaj9

# **Project Expecations**

1.) Problem

I am tackling the problem of predicting pH levels in water. I obtained the data from Kaggle and am using it to predict the pH levels in water. The problem is regression, and I used the mean squared error metric.


2.) Data Preparation

I conducted research to determine which predictors would be most beneficial and found that the provided data were excellent predictors for estimating the pH level. I removed the NA values, split the data, and am confident that it is ready because I took the necessary steps to achieve the best results with machine learning.


3.) Research

KNN: Non-Normalized = 2.590646539055139, Normalized = 2.5872644578743484, Parameter Tuned = 2.488424174563941

W-KNN: Non-Normalized = 2.590646539055139, Normalized = 2.590646539055139, Parameter Tuned = 2.4291322073330472

Decision Tree: Non-Normalized = 4.919338167136293, Normalized = 2.5936967644926123, Parameter Tuned = 4.918701695115075

Random Forest: Non-Normalized = 3.2416640363364944, Normalized = 3.2416640363364944, Parameter Tuned = 2.2744207119881574


4.) Analysis

I discovered that my code was able to predict the pH level of water with the given predictors. I found the process of testing models against each other to determine which one predicts the best. I found that Random Forest with SVR was the best predictor of pH levels. Decision Tree Non-Normalized/SVR were not the best at predicting pH levels. Random Forest likely outperformed other models due to its ability to handle complex relationships within the data and reduce overfitting, making it a robust choice for predicting water pH levels.


5.) Bumps in the Road

I encountered many challenges when working with SVR and in selecting the data I wanted to research. I overcame this by collaborating in groups to discuss our individual projects and conducting additional research.

# **Imports**
"""

# load in the data and necessary libraries
import sklearn
import pandas
from sklearn import tree
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn import neighbors
from sklearn.svm import SVR
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

from google.colab import drive
drive.mount('/content/drive')
phData = pandas.read_csv('/content/drive/MyDrive/CS 167/Google Collab/Datasets/water_potability.csv') # change this to match your dataset directory
phData.head()

"""# **Clean Data**"""

phData = phData.dropna()
phData.head()

"""# **Split Data**"""

target= 'ph'
predictors = phData.columns.drop(target)
train_data, test_data, train_sln, test_sln = train_test_split(phData[predictors], phData[target], test_size = 0.2, random_state=41)
train_data.head()

"""# **Non-Normalized Data**

## **KNN**
"""

# Does KNN for waterData
phKNN = KNeighborsRegressor(n_neighbors=250)

# Trains with trainData
phKNN.fit(train_data,train_sln)

# Predicts using testData
phPredict = phKNN.predict(test_data)

#Shows MSE score
mse = mean_squared_error(test_sln, phPredict)
print(mse)

"""## **W-KNN**"""

# Does W-KNN for ph
phWKNN = KNeighborsRegressor(n_neighbors=250, weights="distance")

# Trains with trainData
phWKNN.fit(train_data,train_sln)

# Predicts using testData
phWPredict = phWKNN.predict(test_data)

#Shows MSE score
mse = mean_squared_error(test_sln, phWPredict)
print(mse)

"""## **Tree Decision**"""

# Creates Decision Tree
phDecisonTree = DecisionTreeRegressor()

# Trains with trainData
phDecisonTree.fit(train_data,train_sln)

# Predicts using testData
phTreePredict = phDecisonTree.predict(test_data)

#Shows MSE score
mse = mean_squared_error(test_sln, phTreePredict)
print(mse)

"""## **Random Forest**"""

#  Creates Decision Tree
phRandomForest = DecisionTreeRegressor(random_state = 41, splitter = 'random', min_samples_split = 10)

# Trains with trainData
phRandomForest.fit(train_data,train_sln)

# Predicts using testData
phRandomForestPredict = phRandomForest.predict(test_data)

#Shows MSE score
mse = mean_squared_error(test_sln, phRandomForestPredict)
print(mse)

"""# **Parameter Tuned**

## **KNN**
"""

# Now, let's use SVC on top of the KNN predictions
svr = SVR()
svr.fit(phPredict.reshape(-1, 1), test_sln)  # Reshape knn_predict to a 2D array

# Predict using the combined model
svcKNNPredict = svr.predict(phPredict.reshape(-1, 1))

# Calculate the mean squared error
mse = mean_squared_error(test_sln, svcKNNPredict)
print(mse)

"""### **KNN Graph**"""

# Create a scatter plot to visualize the predicted values against actual values
plt.scatter(test_sln, svcKNNPredict, alpha=0.5)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs. KNN w/SVR")
plt.show()

"""## **W-KNN**"""

# Now, let's use SVC on top of the KNN predictions
svr = SVR()
svr.fit(phWPredict.reshape(-1, 1), test_sln)  # Reshape knn_predict to a 2D array

# Predict using the combined model
svcWKNNPredict = svr.predict(phWPredict.reshape(-1, 1))

# Calculate the mean squared error
mse = mean_squared_error(test_sln, svcWKNNPredict)
print(mse)

"""### **W-KNN Graph**"""

# Create a scatter plot to visualize the predicted values against actual values
plt.scatter(test_sln, svcWKNNPredict, alpha=0.5)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs. W-KNN SVR")
plt.show()

"""## **Tree Decision**"""

# Creates SVC with Decision Tree kernel
svr_tree = SVR(kernel='decision_function')

svr.fit(train_data, train_sln)  # Fit SVR with Decision Tree predictions

# Combine Decision Tree and SVR predictions
phTreeSVRPredict = svr.predict(test_data)

# Shows MSE score
mse = mean_squared_error(test_sln, phTreeSVRPredict)
print(mse)

"""### **Decision Tree Graph**"""

# Create a scatter plot to visualize the predicted values against actual values
plt.scatter(test_sln, phTreeSVRPredict, alpha=0.5)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs. Decision Tree SVR)")
plt.show()

"""## **Random Forest**"""

# Create Random Forest Regressor with SVR as base estimator
rf_svr = RandomForestRegressor(
    n_estimators=100,  # Number of decision trees in the forest
    random_state=41
)

# Create SVR model to be used as base estimator
svr = SVR(kernel='linear', C=1.0)  # You can change the kernel and C value as needed

# Train with trainData
rf_svr.fit(train_data, train_sln)

# Predict using testData
phSVR_RFPredict = rf_svr.predict(test_data)

# Show MSE score
mse = mean_squared_error(test_sln, phSVR_RFPredict)
print(mse)

"""### **Random Forest Graph**"""

# Create a scatter plot to visualize the predicted values against actual values
plt.scatter(test_sln, phSVR_RFPredict, alpha=0.5)
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs. Random Forest SVR")
plt.show()

"""# **Normalized Data Section**

## **Data is Normalized**
"""

# Normalize the data using Min-Max scaling
scaler = MinMaxScaler()
train_data = scaler.fit_transform(train_data)
test_data = scaler.transform(test_data)

"""## **KNN**"""

# Does KNN for waterData
phNormKNN = KNeighborsRegressor(n_neighbors=250)

# Trains with trainData
phNormKNN.fit(train_data,train_sln)

# Predicts using testData
phNormPredict = phNormKNN.predict(test_data)

#Shows MSE score
mse = mean_squared_error(test_sln, phNormPredict)
print(mse)

"""## **W-KNN**"""

# Does W-KNN for ph
phNormWKNN = KNeighborsRegressor(n_neighbors=250, weights="distance")

# Trains with trainData
phNormWKNN.fit(train_data,train_sln)

# Predicts using testData
phNormWPredict = phNormWKNN.predict(test_data)

#Shows MSE score
mse = mean_squared_error(test_sln, phNormWPredict)
print(mse)

"""## **Tree Decision**"""

# Creates Decision Tree
normDecisonTree = DecisionTreeRegressor()

# Trains with trainData
normDecisonTree.fit(train_data,train_sln)

# Predicts using testData
phNormTreePredict = normDecisonTree.predict(test_data)

#Shows MSE score
mse = mean_squared_error(test_sln, phNormTreePredict)
print(mse)

"""## **Random Forest**"""

#  Creates Decision Tree
phRandomForst = DecisionTreeRegressor(random_state = 41, splitter = 'random', min_samples_split = 10)

# Trains with trainData
phRandomForst.fit(train_data,train_sln)

# Predicts using testData
phNormRandomForestPredict = phRandomForst.predict(test_data)

#Shows MSE score
mse = mean_squared_error(test_sln, phNormRandomForestPredict)
print(mse)